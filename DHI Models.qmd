---
title: "Modeling of Diabetes Health Indicators"
format: html
editor: visual
---

## Purpose of Model

The Purpose of this modeling anlaysis is to use our six choosen predicator variables to find the best model fit that predicates diabetes status. Our metric used to evaluate the models will be log-loss, and the two model types will be  a classification tree and a random forest. Both models will be trained using a 70/30 split with a 5 fold cross-validation. 

```{r}
# Setting up librarys 
library(tidymodels)
library(ranger)
library(parsnip)
```

The first model we will be creating is a classification tree, which is a logistic regresion model commonly used when you have a binary response variable, such as Diabetes status in our case. A classification tree "normally" works by using the most commonly occuring class of observations as the predictions for its regions, and these regions are created by splitting up the predictor space. This allows for numerous predictions to be made . Properly choosing the number of splits is important to control for overfitting (too many splits) and underfitting (too few splits), unforuntantely their is no optimal algorithm to choose splits and the quanity must factor in the benefit/cost of increased processing time. Generally you will start with more splits then prune them down in order make sure that the model is not overfitted (aka does not predict well when using unseen data), and in our case we will be using cross-validation to achieve this. Specfically, by splitting our data up into different folds we can test how pruning impacts predictions when tested on various portions of the data. 

Now that we have that explanation of what a classification tree is, we need to begin the process of building the relevant recipes, specs, and workflow. Before that we should create our CV spilt, and then subset our data by 20% in order to save on processing time. Also of note as we are using a highly imbalanced target variable it will be a good idea to stratify our sample and CV splits.

```{r}
# Data split and 5 fold CV (Will be using this later to get the best possible model)
dhi_split <- initial_split(model_data, prop = 0.7, strata = Diabetes)
dhi_train <- training(dhi_split)
dhi_test <- testing(dhi_split)
dhi_cv <- vfold_cv(dhi_train, 5, strata = Diabetes)

# Creating sample of data
set.seed(113)
subset_data <- model_data |>
  slice_sample(prop = .2)

# Creating sample split and 5 fold CV using the 
sub_split <- initial_split(subset_data, prop = 0.7, strata = Diabetes)
sub_train <- training(sub_split)
sub_test <- testing(sub_split)
sub_cv <- vfold_cv(sub_train, 5, strata = Diabetes)
```

Now will be to create our recipes that will be used by our model. 

```{r}
# Recipe for non subset data
class_recipe <- recipe(Diabetes ~ HighBP + HighChol + BMI + Stroke + PhysActivity + HeartDisease,
                     data = dhi_train) |>
  step_dummy() 
```

```{r}
# Recipe for subset data
subc_recipe <- recipe(Diabetes ~ HighBP + HighChol + BMI + Stroke + PhysActivity + HeartDisease,
                     data = sub_train) |>
  step_dummy(HighBP, HighChol, Stroke, PhysActivity, HeartDisease) 
```

After that we need to create the proper spec for the classification tree model by using set_engine and set_mode. Then we will combine the recipe and spec (model) to create our workflow.

```{r}
# creating classification tree model spec
class_spec <- decision_tree(cost_complexity = tune(),
                            tree_depth = tune(),
                            min_n = tune()) |>
  set_mode("classification") |>
  set_engine("rpart")
```

```{r}
# Creating workflow for non-subsetted data
class_wkflw <- workflow() |>
  add_recipe(class_recipe) |>
  add_model(class_spec)
```

```{r}
# Creating workflopw for subsetted data
subc_wkflw <- workflow() |>
  add_recipe(subc_recipe) |>
  add_model(class_spec)
```

Now that we have created our handy workflow from tidymodels we need to fit our model to the training data using cross-validation. Before we can fit it we need to specify our grid using tune_grid, to make sure our models use our prefered metric log loss (as wells as tune the model), as well as to use grid_regular to specify the cv fold.

```{r}
# Setting grid to be used in fits
class_grid <- grid_regular(cost_complexity(),
                           tree_depth(),
                           min_n(),
                           levels = 5)
```

```{r}
# fitting the model for non-subsetted data
class_fit <- class_wkflw |>
  tune_grid(resamples = dhi_cv,
            grid = class_grid,
            metrics = metric_set(mn_log_loss))

```

```{r}
# fitting the model for subsetted data
subc_fit <- subc_wkflw |>
  tune_grid(resamples = sub_cv,
            grid = class_grid,
            metrics = metric_set(mn_log_loss))

# Collecting metrics to find best fit
subc_fit |>
  collect_metrics() |>
  filter(.metric == "mn_log_loss") |>
  arrange(mean)

# Grabbing best model
best_subc_fit <- subc_fit |>
  select_best(metric = "mn_log_loss")

# Final fit
best_subc_final <- subc_wkflw |>
  finalize_workflow(best_subc_fit) |>
  fit(data = sub_train)
best_subc_final
```

Now that we have found out best fit we should visualize it using rpart.plot.

```{r}
# loading in rpart.plot
library(rpart.plot)

# Extracting fit using extract_fit

# Creating plot of classification tree 
best_subc_final |>
  extract_fit_engine() |>
  rpart.plot(roundint = FALSE, cex = .5)
```

