[
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "Final_Project",
    "section": "",
    "text": "The purpose of this document is to perform exploratory data analysis of the Diabetes Health Indicators Dataset. The Health Indicators Dataset contains content from the The 2015 Behavioral Risk Factor Surveillance System (BRFSS) telephone survey, which is a national CDC ran survey used to collect data on human health behavior and chronic conditions. The content from the survey has been choosen with the specific purpose of analyzing factors related to diabetes.\nOur first steps in analyzing the data will be to read in the document, validate the data, then clean the data. After that we will create numeric and categorical summaries as well as plots of all the variables. This will hopefully help us decide which variables may be important in determining whether someone may have diabetes (or risk of developing).\n\n\n\n# Setting up package librarys \nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n# Reading in CSV file of DHI data\ndhi_data &lt;- read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndhi_data\n\n# A tibble: 253,680 × 22\n   Diabetes_binary HighBP HighChol CholCheck   BMI Smoker Stroke\n             &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1               0      1        1         1    40      1      0\n 2               0      0        0         0    25      1      0\n 3               0      1        1         1    28      0      0\n 4               0      1        0         1    27      0      0\n 5               0      1        1         1    24      0      0\n 6               0      1        1         1    25      1      0\n 7               0      1        0         1    30      1      0\n 8               0      1        1         1    25      1      0\n 9               1      1        1         1    30      1      0\n10               0      0        0         1    24      0      0\n# ℹ 253,670 more rows\n# ℹ 15 more variables: HeartDiseaseorAttack &lt;dbl&gt;, PhysActivity &lt;dbl&gt;,\n#   Fruits &lt;dbl&gt;, Veggies &lt;dbl&gt;, HvyAlcoholConsump &lt;dbl&gt;, AnyHealthcare &lt;dbl&gt;,\n#   NoDocbcCost &lt;dbl&gt;, GenHlth &lt;dbl&gt;, MentHlth &lt;dbl&gt;, PhysHlth &lt;dbl&gt;,\n#   DiffWalk &lt;dbl&gt;, Sex &lt;dbl&gt;, Age &lt;dbl&gt;, Education &lt;dbl&gt;, Income &lt;dbl&gt;\n\n\nCreating a function to speed up the process of converting our binary variables into factors, as R tends to run better when using factors for them.\n\n# Function to change binary variables into factors\nbinary_creator &lt;- function(data){\n  # Grabbing all binary cols\n  binary_cols &lt;- colnames(data)[sapply(data, function(col) all(col %in% c(0, 1)))]\n\n  # removing sex from binary_cols\n  binary_cols &lt;- setdiff(binary_cols, \"Sex\")\n\n  # Creating Binary cols\n  for(var in colnames(data)){\n    if(var %in% binary_cols){\n      data[[var]] &lt;- factor(data[[var]], levels = c(0,1), labels = c(\"No\", \"Yes\"))\n    }\n  }\n  return(data)\n}\n# Running function\ndhi_data &lt;- binary_creator(dhi_data)\n\n# Converting other factor variables \ndhi_data &lt;- dhi_data |&gt;\n  mutate(\n    Sex = factor(Sex, levels = c(0,1), labels = c(\"female\", \"male\")),\n    GenHlth = factor(GenHlth, levels = c(1,2,3,4,5), labels = c(\"excellent\", \"very_good\", \"good\", \"fair\", \"poor\")),\n    Education = factor(Education, levels = c(1,2,3,4,5,6), labels = c(\"no_schooling\", \"1_to_8\", \"9_to_11\", \"12_or_GED\", \"some_college\", \"college_grad\")),\n    Income = factor(Income, levels = c(1,2,3,4,5,6,7,8), labels = c(\"10k_less\", \"15k_less\", \"20k_less\", \"25k_less\", \"35k_less\", \"50k_less\", \"75k_less\", \"75k_more\"))\n  )\n\nNow I am going to rename a few columns in order to make using them easier in terms of referencing them in the code.\n\n# renaming columns \ndhi_data &lt;- dhi_data |&gt;\n  rename(\n    \"Diabetes\" = Diabetes_binary,\n    \"HeartDisease\" = HeartDiseaseorAttack,\n    \"HvyAlch\" = HvyAlcoholConsump,\n    \"Healthcare\" = AnyHealthcare,\n    \"NoDoc\" = NoDocbcCost\n  )\n\nChecking for missing values, this should be zero since the dataset being used has already been cleaned.\n\n# grabbing all missing values\ncolSums(is.na(dhi_data))\n\n    Diabetes       HighBP     HighChol    CholCheck          BMI       Smoker \n           0            0            0            0            0            0 \n      Stroke HeartDisease PhysActivity       Fruits      Veggies      HvyAlch \n           0            0            0            0            0            0 \n  Healthcare        NoDoc      GenHlth     MentHlth     PhysHlth     DiffWalk \n           0            0            0            0            0            0 \n         Sex          Age    Education       Income \n           0            0            0            0 \n\n\nNo missing values were found, which was expected.\n\n\n\nNow we need to create some summary stats and to start we are going to look at the summaries of some numeric variables. We will also be doing this while grouping by the Diabetes column as we want to see how these variables impact Diabetes status.\n\n# Creating function to grab numeric summary stats\nnumeric_summary &lt;- function(data, group_var = \"Diabetes\"){\n  # Selecting numeric vars\n  num_vars &lt;- data |&gt;\n    select(where(is.numeric))\n  \n  # Creating empty list\n  num_sum_list &lt;- list()\n  \n  # Looping summary stats \n  for(num_var in colnames(num_vars)){\n    # Grouping by group_var\n    num_sums &lt;- data |&gt;\n    group_by(across(all_of(group_var)))|&gt;\n    summarize(across(all_of(num_var), .fns = list(\"mean\" = mean, # This will create a named list with .fns\n                                       \"median\" = median,\n                                       \"sd\" = sd,\n                                       \"IQR\" = IQR,\n                                       \"min\" = min,\n                                       \"max\" = max),\n                     .names = \"{.fn}\")) # .fn is function names\n    \n    num_sums &lt;- num_sums |&gt;\n      mutate(variable = num_var)\n    num_sums &lt;- num_sums |&gt; \n      select(variable, everything())\n  \n    num_sum_list[[num_var]] &lt;- num_sums\n  }\n  return(num_sum_list)\n}\n\n# Running function \nnum_sums &lt;- numeric_summary(dhi_data)\n  \n# Printing columns out\nnum_sums\n\n$BMI\n# A tibble: 2 × 8\n  variable Diabetes  mean median    sd   IQR   min   max\n  &lt;chr&gt;    &lt;fct&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 BMI      No        27.8     27  6.29     7    12    98\n2 BMI      Yes       31.9     31  7.36     8    13    98\n\n$MentHlth\n# A tibble: 2 × 8\n  variable Diabetes  mean median    sd   IQR   min   max\n  &lt;chr&gt;    &lt;fct&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 MentHlth No        2.98      0  7.11     2     0    30\n2 MentHlth Yes       4.46      0  8.95     3     0    30\n\n$PhysHlth\n# A tibble: 2 × 8\n  variable Diabetes  mean median    sd   IQR   min   max\n  &lt;chr&gt;    &lt;fct&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 PhysHlth No        3.64      0  8.06     2     0    30\n2 PhysHlth Yes       7.95      1 11.3     15     0    30\n\n$Age\n# A tibble: 2 × 8\n  variable Diabetes  mean median    sd   IQR   min   max\n  &lt;chr&gt;    &lt;fct&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Age      No        7.81      8  3.10     4     1    13\n2 Age      Yes       9.38     10  2.33     3     1    13\n\n\nSome interesting findings can be found based off these results. The first being that the mean and median BMI is lower in the No Diabetes group (27.8/27 to 31.9/31), the mean poor mental health days was lower in the No diabetes group (2.98 to 4.46) but the median was zero for both, the mean was also much higher in the yes diabetes group for number of poor physical health days (3.64 to 7.95) but again the medians were not significantly different (0 to 1), and finally we find that the mean and median were higher in the yes diabetes group for age (7.81/8 to 9.38/10). This could indicate that the MentHlth and PhysHlth variables have data that is skewed with most individuals having no issues most days, but a few individuals with a lot of negative days. To investigate further we should look at some density plots and some bar plots.\n\n# Creating densityplots while colored by Diabetes status\nggplot(dhi_data, aes(x = BMI, fill = Diabetes)) +\n  geom_density(alpha=0.3) +\n  ggtitle(\"BMI by Diabetes Status\") +\n  xlab(\"BMI\")\n\n\n\n\n\n\n\nggplot(dhi_data, aes(x = MentHlth, fill = Diabetes)) +\n  geom_density(alpha=0.3) +\n  ggtitle(\"Number of Poor Mental Health Days by Diabetes Status\") +\n  xlab(\"Poor Mental Health Days\")\n\n\n\n\n\n\n\nggplot(dhi_data, aes(x = PhysHlth, fill = Diabetes)) +\n  geom_density(alpha=0.3) +\n  ggtitle(\"Number of Poor Physical Health Days by Diabetes Status\") +\n  xlab(\"Poor Physical Health Days\")\n\n\n\n\n\n\n\nggplot(dhi_data, aes(x = Age, fill = Diabetes)) +\n  geom_density(alpha=0.3) +\n  ggtitle(\"Age by Diabetes Status\") +\n  xlab(\"Age in Groupings of Five Starting at 18\")\n\n\n\n\n\n\n\n# Creating Barplots for MentHlth, PhysHlth, and Age as they are on scales\nggplot(dhi_data, aes(x = MentHlth)) +\n  geom_bar() +\n  ggtitle(\"Number of Poor Mental Health Days by Diabetes Status\") +\n  xlab(\"Poor Mental Health Days\") +\n  facet_wrap(~Diabetes, scales = \"free_y\") + # Making graphs the same scale to aid comparison\n  scale_y_continuous(expand = c(0, 0))\n\n\n\n\n\n\n\nggplot(dhi_data, aes(x = PhysHlth)) +\n  geom_bar() +\n  ggtitle(\"Number of Poor Physical Health Days by Diabetes Status\") +\n  xlab(\"Poor Physical Health Days\") +\n  facet_wrap(~Diabetes, scales = \"free_y\") + # Making graphs the same scale to aid comparison\n  scale_y_continuous(expand = c(0, 0))\n\n\n\n\n\n\n\nggplot(dhi_data, aes(x = Age)) +\n  geom_bar() +\n  ggtitle(\"Age by Diabetes Status\") +\n  xlab(\"Age in Groupings of Five Starting at 18\") +\n  facet_wrap(~Diabetes, scales = \"free_y\") + # Making graphs the same scale to aid comparison\n  scale_y_continuous(expand = c(0, 0))\n\n\n\n\n\n\n\n\nThe graphs confirm that the distribution of diabetes status is not flat across the numeric variables, meaning that they may have some influence on predicting whether or not someone will have diabetes. It also supported the idea that the data is not uniformally distributed with variables, such as PhysHlth showcasing how most individuals have few or zero poor physical health days, but that 30 poor physical health days is the next most frequent response. The density plot for BMI is also quite interesting as it shows that the distribution for BMI is follows a similar shape between the no and yes groups, but that the Yes group is further to the right than the NO group. Indicating that people with Diabetes are more likely to have higher BMIs.\nThe next step is going to be to create some contingency tables comparing counts of the variables to the diabetes status, we can make a function that will do this for each variable we have turned into a factor.\n\n# Function to create two way contingency tables\nTwoWayMaker &lt;- function(data, var1 = \"Diabetes\"){\n  \n  # Selecting factors\n  fct_vars &lt;- data |&gt;\n    select(where(is.factor)) |&gt;\n    select(-all_of(var1))\n  \n  # Creating empty list to store tables\n  fct_list &lt;- list()\n  \n  # create for loop of factor vars\n  for(fct_var in colnames(fct_vars)){\n    \n    #Create contingency tables\n    fct_tables &lt;- table(data[[var1]], data[[fct_var]])\n    \n    # Create proper names for tables\n    dimnames(fct_tables) &lt;- list(Diabetes = levels(data[[var1]]),\n                                 fct_var = levels(data[[fct_var]]))\n    \n    # Store table in list\n    fct_list[[fct_var]] &lt;- fct_tables\n  }\n  return(fct_list)\n}\n\nTwoWayTables &lt;- TwoWayMaker(dhi_data)\n\nI also want to see the tables as proportions, so lets modify the function in order to do so. Of important note I am going to be using margin = 2 to compare across columns, as that will make the data display as the total percent for each level (column in the table) of a factor rather than against the whole dataset.\n\n# Function to create two way contingency tables\nTwoWayMaker_percent &lt;- function(data, var1 = \"Diabetes\"){\n  \n  # Selecting factors\n  fct_vars &lt;- data |&gt;\n    select(where(is.factor)) |&gt;\n    select(-all_of(var1))\n  \n  # Creating empty list to store tables\n  fct_list &lt;- list()\n  \n  # create for loop of factor vars\n  for(fct_var in colnames(fct_vars)){\n    \n    #Create contingency tables\n    fct_tables &lt;- table(data[[var1]], data[[fct_var]])\n    \n    # Converting to percents\n    fct_tables &lt;- prop.table(fct_tables, margin = 2) * 100\n    \n    # Create proper names for tables\n    dimnames(fct_tables) &lt;- list(Diabetes = levels(data[[var1]]),\n                                 fct_var = levels(data[[fct_var]]))\n    \n    # Store table in list\n    fct_list[[fct_var]] &lt;- fct_tables\n  }\n  return(fct_list)\n}\n\nTwoWayTables_percent &lt;- TwoWayMaker_percent(dhi_data)\nTwoWayTables_percent\n\n$HighBP\n        fct_var\nDiabetes        No       Yes\n     No  93.964833 75.554310\n     Yes  6.035167 24.445690\n\n$HighChol\n        fct_var\nDiabetes        No       Yes\n     No  92.018564 77.985147\n     Yes  7.981436 22.014853\n\n$CholCheck\n        fct_var\nDiabetes        No       Yes\n     No  97.455121 85.625077\n     Yes  2.544879 14.374923\n\n$Smoker\n        fct_var\nDiabetes       No      Yes\n     No  87.94467 83.70707\n     Yes 12.05533 16.29293\n\n$Stroke\n        fct_var\nDiabetes       No      Yes\n     No  86.82022 68.24718\n     Yes 13.17978 31.75282\n\n$HeartDisease\n        fct_var\nDiabetes       No      Yes\n     No  88.04632 67.02800\n     Yes 11.95368 32.97200\n\n$PhysActivity\n        fct_var\nDiabetes       No      Yes\n     No  78.85525 88.38735\n     Yes 21.14475 11.61265\n\n$Fruits\n        fct_var\nDiabetes       No      Yes\n     No  84.20707 87.13906\n     Yes 15.79293 12.86094\n\n$Veggies\n        fct_var\nDiabetes       No      Yes\n     No  82.00213 87.01133\n     Yes 17.99787 12.98867\n\n$HvyAlch\n        fct_var\nDiabetes        No       Yes\n     No  85.584570 94.163861\n     Yes 14.415430  5.836139\n\n$Healthcare\n        fct_var\nDiabetes       No      Yes\n     No  88.54796 85.93900\n     Yes 11.45204 14.06100\n\n$NoDoc\n        fct_var\nDiabetes       No      Yes\n     No  86.39670 82.47635\n     Yes 13.60330 17.52365\n\n$GenHlth\n        fct_var\nDiabetes excellent very_good      good      fair      poor\n     No  97.483388 92.837098 82.210560 68.989547 62.105786\n     Yes  2.516612  7.162902 17.789440 31.010453 37.894214\n\n$DiffWalk\n        fct_var\nDiabetes       No      Yes\n     No  89.46707 69.25366\n     Yes 10.53293 30.74634\n\n$Sex\n        fct_var\nDiabetes   female     male\n     No  87.03213 84.83967\n     Yes 12.96787 15.16033\n\n$Education\n        fct_var\nDiabetes no_schooling    1_to_8   9_to_11 12_or_GED some_college college_grad\n     No     72.988506 70.739550 75.775480 82.364940    85.189529    90.309807\n     Yes    27.011494 29.260450 24.224520 17.635060    14.810471     9.690193\n\n$Income\n        fct_var\nDiabetes  10k_less  15k_less  20k_less  25k_less  35k_less  50k_less  75k_less\n     No  75.710937 73.809726 77.691634 79.865905 82.598617 85.492185 87.817858\n     Yes 24.289063 26.190274 22.308366 20.134095 17.401383 14.507815 12.182142\n        fct_var\nDiabetes  75k_more\n     No  92.039608\n     Yes  7.960392\n\n\nSome notable things can be gleamed from these proportion tables, with HighBP, HighChol, Stroke, HeartDisease, PhysActivity, HvyAlch, DiffWalk, Education, and Income being the most interesting variables. A much higher proportion of individuals with High BP had diabetes than individuals with normal BP (24.45% to 6.04 %), a similar occurrence is seen in individuals with high cholesterol (22.015% to 7.98%), individuals who have had a stroke (31.75% to 13.18%), individuals with heart disease (32.97% to 11.95%), and individuals with difficulty walking (30.74% to 10.53%). Individuals who were physically active in the last 30 days had a lower proportion of diabetes (11.61% to 21.14%), Individuals with higher levels of education had a lower proportion of diabetes (9.69% for college grads to 27.01% for those with no schooling), and individuals with higher incomes had a lower proprtion of diabets (7.96% for those who make 75k or more to 24.28% for those who make less than 10k)\nThe most striking variable has to be that individuals with heavy alcohol usage had an over two times lower rate of diabetes than individuals who do not heavily use alcohol (5.84% to 12.97%), which may indicate that their are some confounding factors in the data. As research has indicated that heavy alcohol usage was found to be correlated with Type Two Diabetes, but this could also be due to the limited options to the response (only &gt;14 or &lt;14 drinks per week), as data has also shown moderate alcohol usage is negatively correlated with Type two diabetes.\nIn order to keep this analysis from getting out of hands I am going to choose only 5 categorical variables to plot (bar plots), as I shall be using those alongside BMI to create a model in another document. The 5 variables I am going to choose are HighBp, HighChol, Stroke, PhysActivity, and HeartDisease. As these are all variables that are likely to have an influence on diabets status.\n\n# Creating faceit box plots\nggplot(dhi_data, aes(x = HighBP)) +\n  geom_bar() +\n  ggtitle(\"BP by Diabetes Status\") +\n  xlab(\"High BP Yes or No\") +\n  facet_wrap(~Diabetes, scales = \"free_y\") + # Making graphs the same scale to aid comparison\n  scale_y_continuous(expand = c(0, 0))\n\n\n\n\n\n\n\nggplot(dhi_data, aes(x = HighChol)) +\n  geom_bar() +\n  ggtitle(\"Cholesterol by Diabetes Status\") +\n  xlab(\"High Cholesterol Yes or No\") +\n  facet_wrap(~Diabetes, scales = \"free_y\") + # Making graphs the same scale to aid comparison\n  scale_y_continuous(expand = c(0, 0))\n\n\n\n\n\n\n\nggplot(dhi_data, aes(x = Stroke)) +\n  geom_bar() +\n  ggtitle(\"Stroke by Diabetes Status\") +\n  xlab(\"Ever had a stroke Yes or No\") +\n  facet_wrap(~Diabetes, scales = \"free_y\") + # Making graphs the same scale to aid comparison\n  scale_y_continuous(expand = c(0, 0))\n\n\n\n\n\n\n\nggplot(dhi_data, aes(x = PhysActivity)) +\n  geom_bar() +\n  ggtitle(\"Physical Activity by Diabetes Status\") +\n  xlab(\"Any Physical Activity in the Last 30 days Yes or No\") +\n  facet_wrap(~Diabetes, scales = \"free_y\") + # Making graphs the same scale to aid comparison\n  scale_y_continuous(expand = c(0, 0))\n\n\n\n\n\n\n\nggplot(dhi_data, aes(x = HeartDisease)) +\n  geom_bar() +\n  ggtitle(\"Heart Disease by Diabetes Status\") +\n  xlab(\"Heart Disease Yes or No\") +\n  facet_wrap(~Diabetes, scales = \"free_y\") + # Making graphs the same scale to aid comparison\n  scale_y_continuous(expand = c(0, 0))\n\n\n\n\n\n\n\n\nThese plots help us visualize the previously discussed proportions we found in the contingency tables, with Physical Activity (Yes) showing to have a higher proportion of No diabetes, while all other variables with a status (Yes) are shown to have higher proportions of diabetes. This indicates that these variables are likely to play an important part in predicting whether or not we should expect a yes or no diabetes status."
  },
  {
    "objectID": "EDA.html#eda-of-diabetes-health-indicators-dataset",
    "href": "EDA.html#eda-of-diabetes-health-indicators-dataset",
    "title": "Final_Project",
    "section": "",
    "text": "The purpose of this document is to perform exploratory data analysis of the Diabetes Health Indicators Dataset. The Health Indicators Dataset contains content from the The 2015 Behavioral Risk Factor Surveillance System (BRFSS) telephone survey, which is a national CDC ran survey used to collect data on human health behavior and chronic conditions. The content from the survey has been choosen with the specific purpose of analyzing factors related to diabetes.\nOur first steps in analyzing the data will be to read in the document, validate the data, then clean the data. After that we will create numeric and categorical summaries as well as plots of all the variables. This will hopefully help us decide which variables may be important in determining whether someone may have diabetes (or risk of developing).\n\n\n\n# Setting up package librarys \nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n# Reading in CSV file of DHI data\ndhi_data &lt;- read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndhi_data\n\n# A tibble: 253,680 × 22\n   Diabetes_binary HighBP HighChol CholCheck   BMI Smoker Stroke\n             &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1               0      1        1         1    40      1      0\n 2               0      0        0         0    25      1      0\n 3               0      1        1         1    28      0      0\n 4               0      1        0         1    27      0      0\n 5               0      1        1         1    24      0      0\n 6               0      1        1         1    25      1      0\n 7               0      1        0         1    30      1      0\n 8               0      1        1         1    25      1      0\n 9               1      1        1         1    30      1      0\n10               0      0        0         1    24      0      0\n# ℹ 253,670 more rows\n# ℹ 15 more variables: HeartDiseaseorAttack &lt;dbl&gt;, PhysActivity &lt;dbl&gt;,\n#   Fruits &lt;dbl&gt;, Veggies &lt;dbl&gt;, HvyAlcoholConsump &lt;dbl&gt;, AnyHealthcare &lt;dbl&gt;,\n#   NoDocbcCost &lt;dbl&gt;, GenHlth &lt;dbl&gt;, MentHlth &lt;dbl&gt;, PhysHlth &lt;dbl&gt;,\n#   DiffWalk &lt;dbl&gt;, Sex &lt;dbl&gt;, Age &lt;dbl&gt;, Education &lt;dbl&gt;, Income &lt;dbl&gt;\n\n\nCreating a function to speed up the process of converting our binary variables into factors, as R tends to run better when using factors for them.\n\n# Function to change binary variables into factors\nbinary_creator &lt;- function(data){\n  # Grabbing all binary cols\n  binary_cols &lt;- colnames(data)[sapply(data, function(col) all(col %in% c(0, 1)))]\n\n  # removing sex from binary_cols\n  binary_cols &lt;- setdiff(binary_cols, \"Sex\")\n\n  # Creating Binary cols\n  for(var in colnames(data)){\n    if(var %in% binary_cols){\n      data[[var]] &lt;- factor(data[[var]], levels = c(0,1), labels = c(\"No\", \"Yes\"))\n    }\n  }\n  return(data)\n}\n# Running function\ndhi_data &lt;- binary_creator(dhi_data)\n\n# Converting other factor variables \ndhi_data &lt;- dhi_data |&gt;\n  mutate(\n    Sex = factor(Sex, levels = c(0,1), labels = c(\"female\", \"male\")),\n    GenHlth = factor(GenHlth, levels = c(1,2,3,4,5), labels = c(\"excellent\", \"very_good\", \"good\", \"fair\", \"poor\")),\n    Education = factor(Education, levels = c(1,2,3,4,5,6), labels = c(\"no_schooling\", \"1_to_8\", \"9_to_11\", \"12_or_GED\", \"some_college\", \"college_grad\")),\n    Income = factor(Income, levels = c(1,2,3,4,5,6,7,8), labels = c(\"10k_less\", \"15k_less\", \"20k_less\", \"25k_less\", \"35k_less\", \"50k_less\", \"75k_less\", \"75k_more\"))\n  )\n\nNow I am going to rename a few columns in order to make using them easier in terms of referencing them in the code.\n\n# renaming columns \ndhi_data &lt;- dhi_data |&gt;\n  rename(\n    \"Diabetes\" = Diabetes_binary,\n    \"HeartDisease\" = HeartDiseaseorAttack,\n    \"HvyAlch\" = HvyAlcoholConsump,\n    \"Healthcare\" = AnyHealthcare,\n    \"NoDoc\" = NoDocbcCost\n  )\n\nChecking for missing values, this should be zero since the dataset being used has already been cleaned.\n\n# grabbing all missing values\ncolSums(is.na(dhi_data))\n\n    Diabetes       HighBP     HighChol    CholCheck          BMI       Smoker \n           0            0            0            0            0            0 \n      Stroke HeartDisease PhysActivity       Fruits      Veggies      HvyAlch \n           0            0            0            0            0            0 \n  Healthcare        NoDoc      GenHlth     MentHlth     PhysHlth     DiffWalk \n           0            0            0            0            0            0 \n         Sex          Age    Education       Income \n           0            0            0            0 \n\n\nNo missing values were found, which was expected.\n\n\n\nNow we need to create some summary stats and to start we are going to look at the summaries of some numeric variables. We will also be doing this while grouping by the Diabetes column as we want to see how these variables impact Diabetes status.\n\n# Creating function to grab numeric summary stats\nnumeric_summary &lt;- function(data, group_var = \"Diabetes\"){\n  # Selecting numeric vars\n  num_vars &lt;- data |&gt;\n    select(where(is.numeric))\n  \n  # Creating empty list\n  num_sum_list &lt;- list()\n  \n  # Looping summary stats \n  for(num_var in colnames(num_vars)){\n    # Grouping by group_var\n    num_sums &lt;- data |&gt;\n    group_by(across(all_of(group_var)))|&gt;\n    summarize(across(all_of(num_var), .fns = list(\"mean\" = mean, # This will create a named list with .fns\n                                       \"median\" = median,\n                                       \"sd\" = sd,\n                                       \"IQR\" = IQR,\n                                       \"min\" = min,\n                                       \"max\" = max),\n                     .names = \"{.fn}\")) # .fn is function names\n    \n    num_sums &lt;- num_sums |&gt;\n      mutate(variable = num_var)\n    num_sums &lt;- num_sums |&gt; \n      select(variable, everything())\n  \n    num_sum_list[[num_var]] &lt;- num_sums\n  }\n  return(num_sum_list)\n}\n\n# Running function \nnum_sums &lt;- numeric_summary(dhi_data)\n  \n# Printing columns out\nnum_sums\n\n$BMI\n# A tibble: 2 × 8\n  variable Diabetes  mean median    sd   IQR   min   max\n  &lt;chr&gt;    &lt;fct&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 BMI      No        27.8     27  6.29     7    12    98\n2 BMI      Yes       31.9     31  7.36     8    13    98\n\n$MentHlth\n# A tibble: 2 × 8\n  variable Diabetes  mean median    sd   IQR   min   max\n  &lt;chr&gt;    &lt;fct&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 MentHlth No        2.98      0  7.11     2     0    30\n2 MentHlth Yes       4.46      0  8.95     3     0    30\n\n$PhysHlth\n# A tibble: 2 × 8\n  variable Diabetes  mean median    sd   IQR   min   max\n  &lt;chr&gt;    &lt;fct&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 PhysHlth No        3.64      0  8.06     2     0    30\n2 PhysHlth Yes       7.95      1 11.3     15     0    30\n\n$Age\n# A tibble: 2 × 8\n  variable Diabetes  mean median    sd   IQR   min   max\n  &lt;chr&gt;    &lt;fct&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Age      No        7.81      8  3.10     4     1    13\n2 Age      Yes       9.38     10  2.33     3     1    13\n\n\nSome interesting findings can be found based off these results. The first being that the mean and median BMI is lower in the No Diabetes group (27.8/27 to 31.9/31), the mean poor mental health days was lower in the No diabetes group (2.98 to 4.46) but the median was zero for both, the mean was also much higher in the yes diabetes group for number of poor physical health days (3.64 to 7.95) but again the medians were not significantly different (0 to 1), and finally we find that the mean and median were higher in the yes diabetes group for age (7.81/8 to 9.38/10). This could indicate that the MentHlth and PhysHlth variables have data that is skewed with most individuals having no issues most days, but a few individuals with a lot of negative days. To investigate further we should look at some density plots and some bar plots.\n\n# Creating densityplots while colored by Diabetes status\nggplot(dhi_data, aes(x = BMI, fill = Diabetes)) +\n  geom_density(alpha=0.3) +\n  ggtitle(\"BMI by Diabetes Status\") +\n  xlab(\"BMI\")\n\n\n\n\n\n\n\nggplot(dhi_data, aes(x = MentHlth, fill = Diabetes)) +\n  geom_density(alpha=0.3) +\n  ggtitle(\"Number of Poor Mental Health Days by Diabetes Status\") +\n  xlab(\"Poor Mental Health Days\")\n\n\n\n\n\n\n\nggplot(dhi_data, aes(x = PhysHlth, fill = Diabetes)) +\n  geom_density(alpha=0.3) +\n  ggtitle(\"Number of Poor Physical Health Days by Diabetes Status\") +\n  xlab(\"Poor Physical Health Days\")\n\n\n\n\n\n\n\nggplot(dhi_data, aes(x = Age, fill = Diabetes)) +\n  geom_density(alpha=0.3) +\n  ggtitle(\"Age by Diabetes Status\") +\n  xlab(\"Age in Groupings of Five Starting at 18\")\n\n\n\n\n\n\n\n# Creating Barplots for MentHlth, PhysHlth, and Age as they are on scales\nggplot(dhi_data, aes(x = MentHlth)) +\n  geom_bar() +\n  ggtitle(\"Number of Poor Mental Health Days by Diabetes Status\") +\n  xlab(\"Poor Mental Health Days\") +\n  facet_wrap(~Diabetes, scales = \"free_y\") + # Making graphs the same scale to aid comparison\n  scale_y_continuous(expand = c(0, 0))\n\n\n\n\n\n\n\nggplot(dhi_data, aes(x = PhysHlth)) +\n  geom_bar() +\n  ggtitle(\"Number of Poor Physical Health Days by Diabetes Status\") +\n  xlab(\"Poor Physical Health Days\") +\n  facet_wrap(~Diabetes, scales = \"free_y\") + # Making graphs the same scale to aid comparison\n  scale_y_continuous(expand = c(0, 0))\n\n\n\n\n\n\n\nggplot(dhi_data, aes(x = Age)) +\n  geom_bar() +\n  ggtitle(\"Age by Diabetes Status\") +\n  xlab(\"Age in Groupings of Five Starting at 18\") +\n  facet_wrap(~Diabetes, scales = \"free_y\") + # Making graphs the same scale to aid comparison\n  scale_y_continuous(expand = c(0, 0))\n\n\n\n\n\n\n\n\nThe graphs confirm that the distribution of diabetes status is not flat across the numeric variables, meaning that they may have some influence on predicting whether or not someone will have diabetes. It also supported the idea that the data is not uniformally distributed with variables, such as PhysHlth showcasing how most individuals have few or zero poor physical health days, but that 30 poor physical health days is the next most frequent response. The density plot for BMI is also quite interesting as it shows that the distribution for BMI is follows a similar shape between the no and yes groups, but that the Yes group is further to the right than the NO group. Indicating that people with Diabetes are more likely to have higher BMIs.\nThe next step is going to be to create some contingency tables comparing counts of the variables to the diabetes status, we can make a function that will do this for each variable we have turned into a factor.\n\n# Function to create two way contingency tables\nTwoWayMaker &lt;- function(data, var1 = \"Diabetes\"){\n  \n  # Selecting factors\n  fct_vars &lt;- data |&gt;\n    select(where(is.factor)) |&gt;\n    select(-all_of(var1))\n  \n  # Creating empty list to store tables\n  fct_list &lt;- list()\n  \n  # create for loop of factor vars\n  for(fct_var in colnames(fct_vars)){\n    \n    #Create contingency tables\n    fct_tables &lt;- table(data[[var1]], data[[fct_var]])\n    \n    # Create proper names for tables\n    dimnames(fct_tables) &lt;- list(Diabetes = levels(data[[var1]]),\n                                 fct_var = levels(data[[fct_var]]))\n    \n    # Store table in list\n    fct_list[[fct_var]] &lt;- fct_tables\n  }\n  return(fct_list)\n}\n\nTwoWayTables &lt;- TwoWayMaker(dhi_data)\n\nI also want to see the tables as proportions, so lets modify the function in order to do so. Of important note I am going to be using margin = 2 to compare across columns, as that will make the data display as the total percent for each level (column in the table) of a factor rather than against the whole dataset.\n\n# Function to create two way contingency tables\nTwoWayMaker_percent &lt;- function(data, var1 = \"Diabetes\"){\n  \n  # Selecting factors\n  fct_vars &lt;- data |&gt;\n    select(where(is.factor)) |&gt;\n    select(-all_of(var1))\n  \n  # Creating empty list to store tables\n  fct_list &lt;- list()\n  \n  # create for loop of factor vars\n  for(fct_var in colnames(fct_vars)){\n    \n    #Create contingency tables\n    fct_tables &lt;- table(data[[var1]], data[[fct_var]])\n    \n    # Converting to percents\n    fct_tables &lt;- prop.table(fct_tables, margin = 2) * 100\n    \n    # Create proper names for tables\n    dimnames(fct_tables) &lt;- list(Diabetes = levels(data[[var1]]),\n                                 fct_var = levels(data[[fct_var]]))\n    \n    # Store table in list\n    fct_list[[fct_var]] &lt;- fct_tables\n  }\n  return(fct_list)\n}\n\nTwoWayTables_percent &lt;- TwoWayMaker_percent(dhi_data)\nTwoWayTables_percent\n\n$HighBP\n        fct_var\nDiabetes        No       Yes\n     No  93.964833 75.554310\n     Yes  6.035167 24.445690\n\n$HighChol\n        fct_var\nDiabetes        No       Yes\n     No  92.018564 77.985147\n     Yes  7.981436 22.014853\n\n$CholCheck\n        fct_var\nDiabetes        No       Yes\n     No  97.455121 85.625077\n     Yes  2.544879 14.374923\n\n$Smoker\n        fct_var\nDiabetes       No      Yes\n     No  87.94467 83.70707\n     Yes 12.05533 16.29293\n\n$Stroke\n        fct_var\nDiabetes       No      Yes\n     No  86.82022 68.24718\n     Yes 13.17978 31.75282\n\n$HeartDisease\n        fct_var\nDiabetes       No      Yes\n     No  88.04632 67.02800\n     Yes 11.95368 32.97200\n\n$PhysActivity\n        fct_var\nDiabetes       No      Yes\n     No  78.85525 88.38735\n     Yes 21.14475 11.61265\n\n$Fruits\n        fct_var\nDiabetes       No      Yes\n     No  84.20707 87.13906\n     Yes 15.79293 12.86094\n\n$Veggies\n        fct_var\nDiabetes       No      Yes\n     No  82.00213 87.01133\n     Yes 17.99787 12.98867\n\n$HvyAlch\n        fct_var\nDiabetes        No       Yes\n     No  85.584570 94.163861\n     Yes 14.415430  5.836139\n\n$Healthcare\n        fct_var\nDiabetes       No      Yes\n     No  88.54796 85.93900\n     Yes 11.45204 14.06100\n\n$NoDoc\n        fct_var\nDiabetes       No      Yes\n     No  86.39670 82.47635\n     Yes 13.60330 17.52365\n\n$GenHlth\n        fct_var\nDiabetes excellent very_good      good      fair      poor\n     No  97.483388 92.837098 82.210560 68.989547 62.105786\n     Yes  2.516612  7.162902 17.789440 31.010453 37.894214\n\n$DiffWalk\n        fct_var\nDiabetes       No      Yes\n     No  89.46707 69.25366\n     Yes 10.53293 30.74634\n\n$Sex\n        fct_var\nDiabetes   female     male\n     No  87.03213 84.83967\n     Yes 12.96787 15.16033\n\n$Education\n        fct_var\nDiabetes no_schooling    1_to_8   9_to_11 12_or_GED some_college college_grad\n     No     72.988506 70.739550 75.775480 82.364940    85.189529    90.309807\n     Yes    27.011494 29.260450 24.224520 17.635060    14.810471     9.690193\n\n$Income\n        fct_var\nDiabetes  10k_less  15k_less  20k_less  25k_less  35k_less  50k_less  75k_less\n     No  75.710937 73.809726 77.691634 79.865905 82.598617 85.492185 87.817858\n     Yes 24.289063 26.190274 22.308366 20.134095 17.401383 14.507815 12.182142\n        fct_var\nDiabetes  75k_more\n     No  92.039608\n     Yes  7.960392\n\n\nSome notable things can be gleamed from these proportion tables, with HighBP, HighChol, Stroke, HeartDisease, PhysActivity, HvyAlch, DiffWalk, Education, and Income being the most interesting variables. A much higher proportion of individuals with High BP had diabetes than individuals with normal BP (24.45% to 6.04 %), a similar occurrence is seen in individuals with high cholesterol (22.015% to 7.98%), individuals who have had a stroke (31.75% to 13.18%), individuals with heart disease (32.97% to 11.95%), and individuals with difficulty walking (30.74% to 10.53%). Individuals who were physically active in the last 30 days had a lower proportion of diabetes (11.61% to 21.14%), Individuals with higher levels of education had a lower proportion of diabetes (9.69% for college grads to 27.01% for those with no schooling), and individuals with higher incomes had a lower proprtion of diabets (7.96% for those who make 75k or more to 24.28% for those who make less than 10k)\nThe most striking variable has to be that individuals with heavy alcohol usage had an over two times lower rate of diabetes than individuals who do not heavily use alcohol (5.84% to 12.97%), which may indicate that their are some confounding factors in the data. As research has indicated that heavy alcohol usage was found to be correlated with Type Two Diabetes, but this could also be due to the limited options to the response (only &gt;14 or &lt;14 drinks per week), as data has also shown moderate alcohol usage is negatively correlated with Type two diabetes.\nIn order to keep this analysis from getting out of hands I am going to choose only 5 categorical variables to plot (bar plots), as I shall be using those alongside BMI to create a model in another document. The 5 variables I am going to choose are HighBp, HighChol, Stroke, PhysActivity, and HeartDisease. As these are all variables that are likely to have an influence on diabets status.\n\n# Creating faceit box plots\nggplot(dhi_data, aes(x = HighBP)) +\n  geom_bar() +\n  ggtitle(\"BP by Diabetes Status\") +\n  xlab(\"High BP Yes or No\") +\n  facet_wrap(~Diabetes, scales = \"free_y\") + # Making graphs the same scale to aid comparison\n  scale_y_continuous(expand = c(0, 0))\n\n\n\n\n\n\n\nggplot(dhi_data, aes(x = HighChol)) +\n  geom_bar() +\n  ggtitle(\"Cholesterol by Diabetes Status\") +\n  xlab(\"High Cholesterol Yes or No\") +\n  facet_wrap(~Diabetes, scales = \"free_y\") + # Making graphs the same scale to aid comparison\n  scale_y_continuous(expand = c(0, 0))\n\n\n\n\n\n\n\nggplot(dhi_data, aes(x = Stroke)) +\n  geom_bar() +\n  ggtitle(\"Stroke by Diabetes Status\") +\n  xlab(\"Ever had a stroke Yes or No\") +\n  facet_wrap(~Diabetes, scales = \"free_y\") + # Making graphs the same scale to aid comparison\n  scale_y_continuous(expand = c(0, 0))\n\n\n\n\n\n\n\nggplot(dhi_data, aes(x = PhysActivity)) +\n  geom_bar() +\n  ggtitle(\"Physical Activity by Diabetes Status\") +\n  xlab(\"Any Physical Activity in the Last 30 days Yes or No\") +\n  facet_wrap(~Diabetes, scales = \"free_y\") + # Making graphs the same scale to aid comparison\n  scale_y_continuous(expand = c(0, 0))\n\n\n\n\n\n\n\nggplot(dhi_data, aes(x = HeartDisease)) +\n  geom_bar() +\n  ggtitle(\"Heart Disease by Diabetes Status\") +\n  xlab(\"Heart Disease Yes or No\") +\n  facet_wrap(~Diabetes, scales = \"free_y\") + # Making graphs the same scale to aid comparison\n  scale_y_continuous(expand = c(0, 0))\n\n\n\n\n\n\n\n\nThese plots help us visualize the previously discussed proportions we found in the contingency tables, with Physical Activity (Yes) showing to have a higher proportion of No diabetes, while all other variables with a status (Yes) are shown to have higher proportions of diabetes. This indicates that these variables are likely to play an important part in predicting whether or not we should expect a yes or no diabetes status."
  },
  {
    "objectID": "EDA.html#link-to-modeling-qmd",
    "href": "EDA.html#link-to-modeling-qmd",
    "title": "Final_Project",
    "section": "Link to Modeling QMD",
    "text": "Link to Modeling QMD\nClick here for the Modeling Page"
  },
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "Modeling of Diabetes Health Indicators",
    "section": "",
    "text": "The Purpose of this modeling analysis is to use our six choosen predictor variables to find the best model fit that predicates diabetes status. Our metric used to evaluate the models will be log-loss, and the two model types will be a classification tree and a random forest. Both models will be trained using a 70/30 split with a 5 fold cross-validation.\n\n\n\n# Setting up librarys \nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.6     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.1\n✔ recipes      1.1.0     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Use suppressPackageStartupMessages() to eliminate package startup messages\n\nlibrary(ranger)\nlibrary(parsnip)\nlibrary(readr) # rendering was having issues with read_csv when linked thought this may help\n\n\n# Reading in CSV file of DHI data\ndhi_data &lt;- read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndhi_data\n\n# A tibble: 253,680 × 22\n   Diabetes_binary HighBP HighChol CholCheck   BMI Smoker Stroke\n             &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1               0      1        1         1    40      1      0\n 2               0      0        0         0    25      1      0\n 3               0      1        1         1    28      0      0\n 4               0      1        0         1    27      0      0\n 5               0      1        1         1    24      0      0\n 6               0      1        1         1    25      1      0\n 7               0      1        0         1    30      1      0\n 8               0      1        1         1    25      1      0\n 9               1      1        1         1    30      1      0\n10               0      0        0         1    24      0      0\n# ℹ 253,670 more rows\n# ℹ 15 more variables: HeartDiseaseorAttack &lt;dbl&gt;, PhysActivity &lt;dbl&gt;,\n#   Fruits &lt;dbl&gt;, Veggies &lt;dbl&gt;, HvyAlcoholConsump &lt;dbl&gt;, AnyHealthcare &lt;dbl&gt;,\n#   NoDocbcCost &lt;dbl&gt;, GenHlth &lt;dbl&gt;, MentHlth &lt;dbl&gt;, PhysHlth &lt;dbl&gt;,\n#   DiffWalk &lt;dbl&gt;, Sex &lt;dbl&gt;, Age &lt;dbl&gt;, Education &lt;dbl&gt;, Income &lt;dbl&gt;\n\n\nCreating factor creating function\n\n# Function to change binary variables into factors\nbinary_creator &lt;- function(data){\n  # Grabbing all binary cols\n  binary_cols &lt;- colnames(data)[sapply(data, function(col) all(col %in% c(0, 1)))]\n\n  # removing sex from binary_cols\n  binary_cols &lt;- setdiff(binary_cols, \"Sex\")\n\n  # Creating Binary cols\n  for(var in colnames(data)){\n    if(var %in% binary_cols){\n      data[[var]] &lt;- factor(data[[var]], levels = c(0,1), labels = c(\"No\", \"Yes\"))\n    }\n  }\n  return(data)\n}\n# Running function\ndhi_data &lt;- binary_creator(dhi_data)\n\n# Converting other factor variables \ndhi_data &lt;- dhi_data |&gt;\n  mutate(\n    Sex = factor(Sex, levels = c(0,1), labels = c(\"female\", \"male\")),\n    GenHlth = factor(GenHlth, levels = c(1,2,3,4,5), labels = c(\"excellent\", \"very_good\", \"good\", \"fair\", \"poor\")),\n    Education = factor(Education, levels = c(1,2,3,4,5,6), labels = c(\"no_schooling\", \"1_to_8\", \"9_to_11\", \"12_or_GED\", \"some_college\", \"college_grad\")),\n    Income = factor(Income, levels = c(1,2,3,4,5,6,7,8), labels = c(\"10k_less\", \"15k_less\", \"20k_less\", \"25k_less\", \"35k_less\", \"50k_less\", \"75k_less\", \"75k_more\"))\n  )\n\nNow I am going to rename a few columns in order to make using them easier in terms of referencing them in the code.\n\n# renaming columns \ndhi_data &lt;- dhi_data |&gt;\n  rename(\n    \"Diabetes\" = Diabetes_binary,\n    \"HeartDisease\" = HeartDiseaseorAttack,\n    \"HvyAlch\" = HvyAlcoholConsump,\n    \"Healthcare\" = AnyHealthcare,\n    \"NoDoc\" = NoDocbcCost\n  )\n\n\n\n\nThe first model we will be creating is a classification tree, which is a logistic regression model commonly used when you have a binary response variable, such as Diabetes status in our case. It is a tree based model meaning that is based upon the a decision tree, which is a representation how different predictor variables can be used to create a prediction of a target value. A classification tree “normally” works by using the most commonly occurring class of observations as the predictions for its regions, and these regions are created by splitting up the predictor space. Properly choosing the number of splits is important to control for overfitting (too many splits) and underfitting (too few splits), unfortunately their is no optimal algorithm to choose splits and the quantity must factor in the benefit/cost of increased processing time. Generally you will start with more splits then prune them down in order make sure that the model is not overfitted (aka does not predict well when using unseen data), and in our case we will be using cross-validation to achieve this. Specifically, by splitting our data up into different folds we can test how pruning impacts predictions when tested on various portions of the data.\nNow that we have that explanation of what a classification tree is, we need to begin the process of building the relevant recipes, specs, and workflow. Before that we should create our CV split, and then subset our data by 20% in order to save on processing time. Also of note as we are using a highly imbalanced target variable it will be a good idea to stratify our sample and CV splits.\n\n# Data split and 5 fold CV (Will be using this later to get the best possible model)\ndhi_split &lt;- initial_split(dhi_data, prop = 0.7, strata = Diabetes)\ndhi_train &lt;- training(dhi_split)\ndhi_test &lt;- testing(dhi_split)\ndhi_cv &lt;- vfold_cv(dhi_train, 5, strata = Diabetes)\n\nNow will be to create our recipes that will be used by our model.\n\n# Recipe for non subset data\nclass_recipe &lt;- recipe(Diabetes ~ HighBP + HighChol + BMI + Stroke + PhysActivity + HeartDisease,\n                     data = dhi_train) |&gt;\n  step_dummy(HighBP, HighChol, Stroke, PhysActivity, HeartDisease) \n\nAfter that we need to create the proper spec for the classification tree model by using set_engine and set_mode. Then we will combine the recipe and spec (model) to create our workflow.\n\n# creating classification tree model spec\nclass_spec &lt;- decision_tree(cost_complexity = tune(),\n                            tree_depth = tune(),\n                            min_n = tune()) |&gt;\n  set_mode(\"classification\") |&gt;\n  set_engine(\"rpart\")\n\n# Creating workflow for non-subsetted data\nclass_wkflw &lt;- workflow() |&gt;\n  add_recipe(class_recipe) |&gt;\n  add_model(class_spec)\n\nNow that we have created our handy workflow from tidymodels we need to fit our model to the training data using cross-validation. Before we can fit it we need to specify our grid using tune_grid, to make sure our models use our prefered metric log loss (as wells as tune the model), as well as to use grid_regular to specify the cv fold.\n\n# Setting grid to be used in fits\nclass_grid &lt;- grid_regular(cost_complexity(),\n                           tree_depth(),\n                           min_n(),\n                           levels = 5)\n\n\n# fitting the model for non-subsetted data\nclass_fit &lt;- class_wkflw |&gt;\n  tune_grid(resamples = dhi_cv,\n            grid = class_grid,\n            metrics = metric_set(mn_log_loss))\n\n# Collecting metrics to find best fit\nclass_fit |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"mn_log_loss\") |&gt;\n  arrange(mean)\n\n# A tibble: 125 × 9\n   cost_complexity tree_depth min_n .metric     .estimator  mean     n std_err\n             &lt;dbl&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n 1    0.0000000001          8     2 mn_log_loss binary     0.344     5 0.00135\n 2    0.0000000178          8     2 mn_log_loss binary     0.344     5 0.00135\n 3    0.00000316            8     2 mn_log_loss binary     0.344     5 0.00135\n 4    0.0000000001          8    11 mn_log_loss binary     0.346     5 0.00183\n 5    0.0000000178          8    11 mn_log_loss binary     0.346     5 0.00183\n 6    0.00000316            8    11 mn_log_loss binary     0.346     5 0.00183\n 7    0.0000000001          8    21 mn_log_loss binary     0.346     5 0.00142\n 8    0.0000000178          8    21 mn_log_loss binary     0.346     5 0.00142\n 9    0.00000316            8    21 mn_log_loss binary     0.346     5 0.00142\n10    0.0000000001         11    30 mn_log_loss binary     0.347     5 0.00139\n# ℹ 115 more rows\n# ℹ 1 more variable: .config &lt;chr&gt;\n\n# Grabbing best model\nbest_class_fit &lt;- class_fit |&gt;\n  select_best(metric = \"mn_log_loss\")\n\n# Final fit\nbest_class_final &lt;- class_wkflw |&gt;\n  finalize_workflow(best_class_fit) |&gt;\n  last_fit(dhi_split, metrics = metric_set(mn_log_loss))\nbest_class_final\n\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits                 id            .metrics .notes   .predictions .workflow \n  &lt;list&gt;                 &lt;chr&gt;         &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n1 &lt;split [177575/76105]&gt; train/test s… &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;\n\n\nNow that we have found out best fit we should visualize it using rpart.plot.\n\n# loading in rpart.plot\nlibrary(rpart.plot)\n\n# Creating plot of classification tree \nbest_class_final |&gt;\n  extract_fit_engine() |&gt;\n  rpart.plot(roundint = FALSE, cex = .6)\n\nWarning: labs do not fit even at cex 0.15, there may be some overplotting\n\n\n\n\n\n\n\n\n\nBased off this plot we can tell that HighBP is the most important predictor as it is the variable used to start the decision process, with BMI being the next most important node.\n\n\n\nThe next model we will be creating is a Random Forest model which is similar to the classification tree, in that the model builds a number of splits of the predictors to find the best model, but instead of using the most common class of observation it instead uses a random sample of the predictors. This random sample is represented by mtry, and a fresh sample of the predictors is taken every time the tree makes a split. Another big difference is that similar to a bagged tree the model also uses bootstrapping to aggregate results from various tree to make a final prediction, but as it does not use all predictor variables every time it has the potential to reduce the risk of overfitting the model (bagged trees are more correlated, due to being influenced by strong predictors). The random forest model is less prone to over fitting than the classification tree model, and can potentially achieve greater accuracy due to this than a classification tree.\n\n# creating model \nrf_spec &lt;- rand_forest(mtry = tune()) |&gt;\n  set_engine(\"ranger\", importance = \"impurity\") |&gt;\n  set_mode(\"classification\")\n\n# Creating non-subsetted workflow\nrf_wkflw &lt;- workflow() |&gt;\n  add_recipe(class_recipe) |&gt; # same recipe from classification tree will work here\n  add_model(rf_spec)\n\n\n# fitting non-subsetted data with tune_grid and grid_regular\nrf_fit &lt;- rf_wkflw |&gt;\n  tune_grid(resamples = dhi_cv,\n             grid = 10,\n             metrics = metric_set(mn_log_loss))\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n# Grabbing metrics for log loss\nrf_fit |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"mn_log_loss\") |&gt;\n  arrange(mean)\n\n# A tibble: 6 × 7\n   mtry .metric     .estimator  mean     n  std_err .config             \n  &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1     3 mn_log_loss binary     0.338     5 0.000664 Preprocessor1_Model2\n2     4 mn_log_loss binary     0.338     5 0.000659 Preprocessor1_Model1\n3     2 mn_log_loss binary     0.339     5 0.000624 Preprocessor1_Model5\n4     5 mn_log_loss binary     0.340     5 0.000719 Preprocessor1_Model6\n5     6 mn_log_loss binary     0.348     5 0.00162  Preprocessor1_Model3\n6     1 mn_log_loss binary     0.351     5 0.000343 Preprocessor1_Model4\n\n# Grabbing best one\nbest_rf_param &lt;- rf_fit |&gt;\n  select_best(metric = \"mn_log_loss\")\n\n# Using finalize_workflow to create final model\nrf_final &lt;- rf_wkflw |&gt;\n  finalize_workflow(best_rf_param) |&gt;\n  last_fit(dhi_split, metrics = metric_set(mn_log_loss))\n\nNow that we have created out final fit we should go ahead and plot the data, and to do so we will create a variable importance plot (VIP). This will allow us to visualize the most important variables for our model.\n\n# Extracting fit using extract_fit_engine\nrf_final_model &lt;- extract_fit_engine(rf_final)\n\n# Creating VIP plot\ntibble(term = names(rf_final_model$variable.importance),\n       value = rf_final_model$variable.importance) |&gt;\n  arrange(value) |&gt;\n  mutate(term = factor(term, levels = term)) |&gt;\n  ggplot(aes(x = term, y = value)) +\n  geom_bar(stat =\"identity\") +\n  coord_flip()\n\n\n\n\n\n\n\n\nAs we can see based off this model once again a yes status to HighBP and BMI are the most important predictors for diabetes status, with a yes status to HeartDisease and HighChol also being important for the model. PhysActivity yes status and stroke yes status do not appear to have a high level of importance in this model.\n\n\n\nNow that we have created and selected the best models we should compare the metrics in order to choose the best one that will be used on the whole data set.\n\n# Metrics for Classification Tree Model\nbest_class_final |&gt;\n  collect_metrics()\n\n# A tibble: 1 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 mn_log_loss binary         0.349 Preprocessor1_Model1\n\n# Metrics for Random Forest Model\nrf_final |&gt;\n  collect_metrics()\n\n# A tibble: 1 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 mn_log_loss binary         0.339 Preprocessor1_Model1\n\n\nBased off these results the Random Forest Model is the best model as it has the lower log loss out of the two models. This was expected as generally speaking random forest models can be expected to have a higher accuracy than a classification tree. The final step now is to take the model and fit it to the whole data set.\n\n# fitting the best model \nbest_overall_model &lt;- rf_wkflw |&gt;\n  finalize_workflow(best_rf_param) |&gt;\n  fit(dhi_data)\nbest_overall_model\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~3L,      x), importance = ~\"impurity\", num.threads = 1, verbose = FALSE,      seed = sample.int(10^5, 1), probability = TRUE) \n\nType:                             Probability estimation \nNumber of trees:                  500 \nSample size:                      253680 \nNumber of independent variables:  6 \nMtry:                             3 \nTarget node size:                 10 \nVariable importance mode:         impurity \nSplitrule:                        gini \nOOB prediction error (Brier s.):  0.1028644 \n\n\nNow we have the knowledge of which model is the best we can use this to create our API, as we will be replicating this process and creating an interactive medium to create predictions of diabetes status."
  },
  {
    "objectID": "Modeling.html#purpose-of-model",
    "href": "Modeling.html#purpose-of-model",
    "title": "Modeling of Diabetes Health Indicators",
    "section": "",
    "text": "The Purpose of this modeling analysis is to use our six choosen predictor variables to find the best model fit that predicates diabetes status. Our metric used to evaluate the models will be log-loss, and the two model types will be a classification tree and a random forest. Both models will be trained using a 70/30 split with a 5 fold cross-validation.\n\n\n\n# Setting up librarys \nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.6     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.1\n✔ recipes      1.1.0     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Use suppressPackageStartupMessages() to eliminate package startup messages\n\nlibrary(ranger)\nlibrary(parsnip)\nlibrary(readr) # rendering was having issues with read_csv when linked thought this may help\n\n\n# Reading in CSV file of DHI data\ndhi_data &lt;- read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndhi_data\n\n# A tibble: 253,680 × 22\n   Diabetes_binary HighBP HighChol CholCheck   BMI Smoker Stroke\n             &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1               0      1        1         1    40      1      0\n 2               0      0        0         0    25      1      0\n 3               0      1        1         1    28      0      0\n 4               0      1        0         1    27      0      0\n 5               0      1        1         1    24      0      0\n 6               0      1        1         1    25      1      0\n 7               0      1        0         1    30      1      0\n 8               0      1        1         1    25      1      0\n 9               1      1        1         1    30      1      0\n10               0      0        0         1    24      0      0\n# ℹ 253,670 more rows\n# ℹ 15 more variables: HeartDiseaseorAttack &lt;dbl&gt;, PhysActivity &lt;dbl&gt;,\n#   Fruits &lt;dbl&gt;, Veggies &lt;dbl&gt;, HvyAlcoholConsump &lt;dbl&gt;, AnyHealthcare &lt;dbl&gt;,\n#   NoDocbcCost &lt;dbl&gt;, GenHlth &lt;dbl&gt;, MentHlth &lt;dbl&gt;, PhysHlth &lt;dbl&gt;,\n#   DiffWalk &lt;dbl&gt;, Sex &lt;dbl&gt;, Age &lt;dbl&gt;, Education &lt;dbl&gt;, Income &lt;dbl&gt;\n\n\nCreating factor creating function\n\n# Function to change binary variables into factors\nbinary_creator &lt;- function(data){\n  # Grabbing all binary cols\n  binary_cols &lt;- colnames(data)[sapply(data, function(col) all(col %in% c(0, 1)))]\n\n  # removing sex from binary_cols\n  binary_cols &lt;- setdiff(binary_cols, \"Sex\")\n\n  # Creating Binary cols\n  for(var in colnames(data)){\n    if(var %in% binary_cols){\n      data[[var]] &lt;- factor(data[[var]], levels = c(0,1), labels = c(\"No\", \"Yes\"))\n    }\n  }\n  return(data)\n}\n# Running function\ndhi_data &lt;- binary_creator(dhi_data)\n\n# Converting other factor variables \ndhi_data &lt;- dhi_data |&gt;\n  mutate(\n    Sex = factor(Sex, levels = c(0,1), labels = c(\"female\", \"male\")),\n    GenHlth = factor(GenHlth, levels = c(1,2,3,4,5), labels = c(\"excellent\", \"very_good\", \"good\", \"fair\", \"poor\")),\n    Education = factor(Education, levels = c(1,2,3,4,5,6), labels = c(\"no_schooling\", \"1_to_8\", \"9_to_11\", \"12_or_GED\", \"some_college\", \"college_grad\")),\n    Income = factor(Income, levels = c(1,2,3,4,5,6,7,8), labels = c(\"10k_less\", \"15k_less\", \"20k_less\", \"25k_less\", \"35k_less\", \"50k_less\", \"75k_less\", \"75k_more\"))\n  )\n\nNow I am going to rename a few columns in order to make using them easier in terms of referencing them in the code.\n\n# renaming columns \ndhi_data &lt;- dhi_data |&gt;\n  rename(\n    \"Diabetes\" = Diabetes_binary,\n    \"HeartDisease\" = HeartDiseaseorAttack,\n    \"HvyAlch\" = HvyAlcoholConsump,\n    \"Healthcare\" = AnyHealthcare,\n    \"NoDoc\" = NoDocbcCost\n  )\n\n\n\n\nThe first model we will be creating is a classification tree, which is a logistic regression model commonly used when you have a binary response variable, such as Diabetes status in our case. It is a tree based model meaning that is based upon the a decision tree, which is a representation how different predictor variables can be used to create a prediction of a target value. A classification tree “normally” works by using the most commonly occurring class of observations as the predictions for its regions, and these regions are created by splitting up the predictor space. Properly choosing the number of splits is important to control for overfitting (too many splits) and underfitting (too few splits), unfortunately their is no optimal algorithm to choose splits and the quantity must factor in the benefit/cost of increased processing time. Generally you will start with more splits then prune them down in order make sure that the model is not overfitted (aka does not predict well when using unseen data), and in our case we will be using cross-validation to achieve this. Specifically, by splitting our data up into different folds we can test how pruning impacts predictions when tested on various portions of the data.\nNow that we have that explanation of what a classification tree is, we need to begin the process of building the relevant recipes, specs, and workflow. Before that we should create our CV split, and then subset our data by 20% in order to save on processing time. Also of note as we are using a highly imbalanced target variable it will be a good idea to stratify our sample and CV splits.\n\n# Data split and 5 fold CV (Will be using this later to get the best possible model)\ndhi_split &lt;- initial_split(dhi_data, prop = 0.7, strata = Diabetes)\ndhi_train &lt;- training(dhi_split)\ndhi_test &lt;- testing(dhi_split)\ndhi_cv &lt;- vfold_cv(dhi_train, 5, strata = Diabetes)\n\nNow will be to create our recipes that will be used by our model.\n\n# Recipe for non subset data\nclass_recipe &lt;- recipe(Diabetes ~ HighBP + HighChol + BMI + Stroke + PhysActivity + HeartDisease,\n                     data = dhi_train) |&gt;\n  step_dummy(HighBP, HighChol, Stroke, PhysActivity, HeartDisease) \n\nAfter that we need to create the proper spec for the classification tree model by using set_engine and set_mode. Then we will combine the recipe and spec (model) to create our workflow.\n\n# creating classification tree model spec\nclass_spec &lt;- decision_tree(cost_complexity = tune(),\n                            tree_depth = tune(),\n                            min_n = tune()) |&gt;\n  set_mode(\"classification\") |&gt;\n  set_engine(\"rpart\")\n\n# Creating workflow for non-subsetted data\nclass_wkflw &lt;- workflow() |&gt;\n  add_recipe(class_recipe) |&gt;\n  add_model(class_spec)\n\nNow that we have created our handy workflow from tidymodels we need to fit our model to the training data using cross-validation. Before we can fit it we need to specify our grid using tune_grid, to make sure our models use our prefered metric log loss (as wells as tune the model), as well as to use grid_regular to specify the cv fold.\n\n# Setting grid to be used in fits\nclass_grid &lt;- grid_regular(cost_complexity(),\n                           tree_depth(),\n                           min_n(),\n                           levels = 5)\n\n\n# fitting the model for non-subsetted data\nclass_fit &lt;- class_wkflw |&gt;\n  tune_grid(resamples = dhi_cv,\n            grid = class_grid,\n            metrics = metric_set(mn_log_loss))\n\n# Collecting metrics to find best fit\nclass_fit |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"mn_log_loss\") |&gt;\n  arrange(mean)\n\n# A tibble: 125 × 9\n   cost_complexity tree_depth min_n .metric     .estimator  mean     n std_err\n             &lt;dbl&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n 1    0.0000000001          8     2 mn_log_loss binary     0.344     5 0.00135\n 2    0.0000000178          8     2 mn_log_loss binary     0.344     5 0.00135\n 3    0.00000316            8     2 mn_log_loss binary     0.344     5 0.00135\n 4    0.0000000001          8    11 mn_log_loss binary     0.346     5 0.00183\n 5    0.0000000178          8    11 mn_log_loss binary     0.346     5 0.00183\n 6    0.00000316            8    11 mn_log_loss binary     0.346     5 0.00183\n 7    0.0000000001          8    21 mn_log_loss binary     0.346     5 0.00142\n 8    0.0000000178          8    21 mn_log_loss binary     0.346     5 0.00142\n 9    0.00000316            8    21 mn_log_loss binary     0.346     5 0.00142\n10    0.0000000001         11    30 mn_log_loss binary     0.347     5 0.00139\n# ℹ 115 more rows\n# ℹ 1 more variable: .config &lt;chr&gt;\n\n# Grabbing best model\nbest_class_fit &lt;- class_fit |&gt;\n  select_best(metric = \"mn_log_loss\")\n\n# Final fit\nbest_class_final &lt;- class_wkflw |&gt;\n  finalize_workflow(best_class_fit) |&gt;\n  last_fit(dhi_split, metrics = metric_set(mn_log_loss))\nbest_class_final\n\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits                 id            .metrics .notes   .predictions .workflow \n  &lt;list&gt;                 &lt;chr&gt;         &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n1 &lt;split [177575/76105]&gt; train/test s… &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;\n\n\nNow that we have found out best fit we should visualize it using rpart.plot.\n\n# loading in rpart.plot\nlibrary(rpart.plot)\n\n# Creating plot of classification tree \nbest_class_final |&gt;\n  extract_fit_engine() |&gt;\n  rpart.plot(roundint = FALSE, cex = .6)\n\nWarning: labs do not fit even at cex 0.15, there may be some overplotting\n\n\n\n\n\n\n\n\n\nBased off this plot we can tell that HighBP is the most important predictor as it is the variable used to start the decision process, with BMI being the next most important node.\n\n\n\nThe next model we will be creating is a Random Forest model which is similar to the classification tree, in that the model builds a number of splits of the predictors to find the best model, but instead of using the most common class of observation it instead uses a random sample of the predictors. This random sample is represented by mtry, and a fresh sample of the predictors is taken every time the tree makes a split. Another big difference is that similar to a bagged tree the model also uses bootstrapping to aggregate results from various tree to make a final prediction, but as it does not use all predictor variables every time it has the potential to reduce the risk of overfitting the model (bagged trees are more correlated, due to being influenced by strong predictors). The random forest model is less prone to over fitting than the classification tree model, and can potentially achieve greater accuracy due to this than a classification tree.\n\n# creating model \nrf_spec &lt;- rand_forest(mtry = tune()) |&gt;\n  set_engine(\"ranger\", importance = \"impurity\") |&gt;\n  set_mode(\"classification\")\n\n# Creating non-subsetted workflow\nrf_wkflw &lt;- workflow() |&gt;\n  add_recipe(class_recipe) |&gt; # same recipe from classification tree will work here\n  add_model(rf_spec)\n\n\n# fitting non-subsetted data with tune_grid and grid_regular\nrf_fit &lt;- rf_wkflw |&gt;\n  tune_grid(resamples = dhi_cv,\n             grid = 10,\n             metrics = metric_set(mn_log_loss))\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n# Grabbing metrics for log loss\nrf_fit |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"mn_log_loss\") |&gt;\n  arrange(mean)\n\n# A tibble: 6 × 7\n   mtry .metric     .estimator  mean     n  std_err .config             \n  &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1     3 mn_log_loss binary     0.338     5 0.000664 Preprocessor1_Model2\n2     4 mn_log_loss binary     0.338     5 0.000659 Preprocessor1_Model1\n3     2 mn_log_loss binary     0.339     5 0.000624 Preprocessor1_Model5\n4     5 mn_log_loss binary     0.340     5 0.000719 Preprocessor1_Model6\n5     6 mn_log_loss binary     0.348     5 0.00162  Preprocessor1_Model3\n6     1 mn_log_loss binary     0.351     5 0.000343 Preprocessor1_Model4\n\n# Grabbing best one\nbest_rf_param &lt;- rf_fit |&gt;\n  select_best(metric = \"mn_log_loss\")\n\n# Using finalize_workflow to create final model\nrf_final &lt;- rf_wkflw |&gt;\n  finalize_workflow(best_rf_param) |&gt;\n  last_fit(dhi_split, metrics = metric_set(mn_log_loss))\n\nNow that we have created out final fit we should go ahead and plot the data, and to do so we will create a variable importance plot (VIP). This will allow us to visualize the most important variables for our model.\n\n# Extracting fit using extract_fit_engine\nrf_final_model &lt;- extract_fit_engine(rf_final)\n\n# Creating VIP plot\ntibble(term = names(rf_final_model$variable.importance),\n       value = rf_final_model$variable.importance) |&gt;\n  arrange(value) |&gt;\n  mutate(term = factor(term, levels = term)) |&gt;\n  ggplot(aes(x = term, y = value)) +\n  geom_bar(stat =\"identity\") +\n  coord_flip()\n\n\n\n\n\n\n\n\nAs we can see based off this model once again a yes status to HighBP and BMI are the most important predictors for diabetes status, with a yes status to HeartDisease and HighChol also being important for the model. PhysActivity yes status and stroke yes status do not appear to have a high level of importance in this model.\n\n\n\nNow that we have created and selected the best models we should compare the metrics in order to choose the best one that will be used on the whole data set.\n\n# Metrics for Classification Tree Model\nbest_class_final |&gt;\n  collect_metrics()\n\n# A tibble: 1 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 mn_log_loss binary         0.349 Preprocessor1_Model1\n\n# Metrics for Random Forest Model\nrf_final |&gt;\n  collect_metrics()\n\n# A tibble: 1 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 mn_log_loss binary         0.339 Preprocessor1_Model1\n\n\nBased off these results the Random Forest Model is the best model as it has the lower log loss out of the two models. This was expected as generally speaking random forest models can be expected to have a higher accuracy than a classification tree. The final step now is to take the model and fit it to the whole data set.\n\n# fitting the best model \nbest_overall_model &lt;- rf_wkflw |&gt;\n  finalize_workflow(best_rf_param) |&gt;\n  fit(dhi_data)\nbest_overall_model\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~3L,      x), importance = ~\"impurity\", num.threads = 1, verbose = FALSE,      seed = sample.int(10^5, 1), probability = TRUE) \n\nType:                             Probability estimation \nNumber of trees:                  500 \nSample size:                      253680 \nNumber of independent variables:  6 \nMtry:                             3 \nTarget node size:                 10 \nVariable importance mode:         impurity \nSplitrule:                        gini \nOOB prediction error (Brier s.):  0.1028644 \n\n\nNow we have the knowledge of which model is the best we can use this to create our API, as we will be replicating this process and creating an interactive medium to create predictions of diabetes status."
  }
]